---
title: "Homework 1"
author: "PSTAT 115, Fall 2020"
date: "__Due on Sunday, October 18, 2020 at 11:59 pm__"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
library(testthat)
library(tidyverse)
library(patchwork)
eval <- TRUE
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center', 
                      eval=eval)
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')
r = function(x, digits=2){ round(x, digits=digits) }

```

__Note:__ If you are working with a partner, please submit only one homework per group with both names and whether you are taking the course for graduate credit or not.  Submit your Rmarkdown (.Rmd) and the compiled pdf on Gradescope in a zip file.  Include any addition files (e.g. scanned handwritten solutions) in zip file with the pdf. 
 
# Text Analysis of JK Rowling's Harry Potter Series

# Question 1
You are interested in studying the writing style and tone used by JK Rowling (JKR for short), the author of the popular Harry Potter series.  You select a random sample of chapters of size $n$ from all of JKR's books.  You are interested in the rate at which JKR uses the word _dark_ in her writing, so you count how many times the word _dark_ appears in each chapter in your sample, $(y_1,...,y_n)$. In this set-up, $y_i$ is the  number of times the word _dark_ appeared in the $i$-th randomly sampled chapter. In this context, the population of interest is all chapters written by JRK and the population quantity of interest (the estimand) is the rate at which JKR uses the word _dark_. The sampling units are individual chapters. Note: this assignment is partially based on text analysis package known as [tidytext](https://www.tidytextmining.com/tidytext.html]).  You can read more about tidytext [here](https://uc-r.github.io/tidy_text).  



## 1a.
Model: let $Y_i$ denote the quantity that captures the number of times the word _dark_ appears in   the $i$-th chapter.  As a first approximation, it is reasonable to model the number of times _dark_ appears in a given chapter using a  Poisson distribution. _Reminder:_ Poisson distributions are for integer outcomes and useful for events that occur independently and at a constant rate.  Let's assume that the quantities $Y_1,...Y_n$ are independent and identically distributed (IID) according to a Poisson distribution with unknown parameter $\lambda$,
        $$p(Y_i=y_i\mid\lambda) = \hbox{Poisson} (y_i \mid\lambda) \quad \hbox{ for } \quad     i=1,...,n.$$

Write the likelihood $L(\lambda)$ for a generic sample of $n$ chapters, $(y_1,...,y_n)$. Simplify as much as possible (i.e. get rid of any multiplicative constants)  


$$\mathcal{L} ( \lambda ) = \mathbb{P} ({Y_i=y_i\mid\lambda})=\prod_{i=1}^{n} \frac{e^{-\lambda} \lambda  ^{y_i}}{y_i!}$$

$$\mathcal{L} ( \lambda ) =  \frac{e^{-\lambda n} {\lambda  ^{ \sum\nolimits_{i=1}^{n} y_i}}}{ \prod_{i=1}^{n} y_i!} \propto e^{-\lambda n} {\lambda  ^{ \sum\nolimits_{i=1}^{n} y_i}} $$

    
## 1b. 

Write the log-likelihood $\ell(\lambda)$ for a generic sample of $n$ articles, $(y_1,...,y_n)$.  Simplify as much as possible.  Use this to compute the maximum likelihood estimate for the rate parameter of the Poisson distribution.    

$$l({\lambda})=\log({\mathcal{L} ( \lambda )})=-\lambda n+ \sum\nolimits_{i=1}^{n} y_i \log({\lambda})$$ 


By taking the partial derivative in respect to $\lambda$ and equate it to 0:
$$ \frac{\mathrm{d} l({\lambda}) }{\mathrm{d} \lambda}=-n+\frac{\sum\nolimits_{i=1}^{n} y_i}{\lambda} = 0$$
$$\hat \lambda=\frac{\sum\nolimits_{i=1}^{n} y_i}{n}=\bar{y}$$
So the maximum likelihood estimate for the parameter is $\bar{y}$. 

From now on, we'll focus on JKR's writing style in the last Harry Potter book, _The Deathly Hallows_.  This book has 37 chapters. Below is the code for counting the number of times _dark_ appears in each chapter of _The Deathly Hallows_ .  We use the `tidytext` R package which includes functions that parse large text files into word counts.  The code below creates a vector of length 37 which has the number of times the word _dark_ was used in that chapter (see https://uc-r.github.io/tidy_text for more on parsing text with `tidytext`)
    
```{r parse_text}
library(tidyverse)      # data manipulation & plotting
library(stringr)        # text cleaning and regular expressions
library(tidytext)       # provides additional text mining functions
library(harrypotter)    # text for the seven novels of the Harry Potter series

text_tb <- tibble(chapter = seq_along(deathly_hallows),
                  text = deathly_hallows)
tokens <- text_tb %>% unnest_tokens(word, text)
word_counts <- tokens %>% group_by(chapter) %>% 
  count(word, sort = TRUE) %>% ungroup
word_counts_mat <- word_counts %>% spread(key=word, value=n, fill=0)

dark_counts <- word_counts_mat$dark

text_tb <- tibble(chapter = seq_along(deathly_hallows),
                  text = deathly_hallows)
tokens <- text_tb %>% unnest_tokens(word, text)
word_counts <- tokens %>% group_by(chapter) %>% 
  count(word, sort = TRUE) %>% ungroup
word_counts_mat <- word_counts %>% spread(key=word, value=n, fill=0)
```

## 1c.
Make a bar plot where the heights are the counts of the word _dark_ and the x-axis is the chapter.


```{r, dependson="parse_text", out.width="50%"}
df_darkCounts = data.frame(count = dark_counts, 
                            chapter = text_tb$chapter)


df_darkCounts %>%
  ggplot(aes(x = chapter, y = count)) +
  geom_histogram(stat = 'identity', binwidth = 0.5)
```



## 1d.
Plot the log-likelihood of the Poisson rate of _dark_ usage in R using the data in `dark_counts`.  Then use `dark_counts` to compute the maximum likelihood estimate of the rate of the usage of the word _dark_ in The Deathly Hallows.  Mark this maximum on the log-likelihood plot with a vertical line (use `abline` if you make the plot in base R or `geom_vline` if you prefer `ggplot`).  

```{r}
n = 37 # the lenth of charpter 

L <- function(lambda){ #Log-Likelihood Function
  -lambda*n+sum(dark_counts)*log(lambda)
  #+rep(sum(log(factorial(dark_counts))))
}

lambda_mle = mean(dark_counts)
lambda_mle

df_logLikelihood = data.frame(lambda = mean(dark_counts))
                              
ggplot(data = df_logLikelihood,mapping = aes(x=lambda)) +
  stat_function(fun = L)+ 
  xlim(1,40)+scale_y_continuous(name = "Likelihood")+
  geom_vline(xintercept = lambda_mle, color = 'red') 
```




# Question 2

For the previous problem, when computing the rate of _dark_ usage, we were implicitly assuming each chapter had the same length.  Remember that for $Y_i \sim \text{Poisson}(\lambda)$, $E[Y_i] = \lambda$ for each chapter, that is, the average number of occurrences of _dark_ is the same in each chapter.  Obviously this isn't a great assumption, since the lengths of the chapters vary; longer chapters should be more likely to have more occurrences of the word.  We can augment the model by considering properties of the Poisson distribution.  The Poisson is often used to express the probability of a given number of events occurring for a fixed ``exposure''.  As a useful example of the role of the exposure term, when counting then number of events that happen in a set length of time, we to need to account for the total time that we are observing events.  For this text example, the exposure is not time, but rather corresponds to the total length of the chapter.  

We will again let $(y_1,...,y_n)$ represent counts of the word _dark_. In addition, we now count the total number of words in each each chapter $(\nu_1,...,\nu_n)$ and use this as our exposure.  Let $Y_i$ denote the random variable for the counts of the word _dark_ in a chapter with $\nu_i$ words.  Let's assume that the quantities $Y_1,...Y_n$ are independent and identically distributed (IID) according to a Poisson distribution with unknown parameter $\lambda \cdot \frac{\nu_i}{1000}$,
$$
 p(Yi=y_i\mid \nu_i, 1000) = \hbox{Poisson} (y_i \mid\lambda \cdot \frac{\nu_i}{1000}) \quad \hbox{ for } \quad i=1,...,n.
$$

In the code below, `chapter_lengths` is a vector storing the length of each chapter in words.  

```{r chapter_lengths, dependson="parse_text", out.width="50%"}
chapter_lengths <- word_counts %>% group_by(chapter) %>% 
summarize(chapter_length = sum(n)) %>% 
ungroup %>% select(chapter_length) %>% unlist %>% as.numeric
```


## 2a.

What is the interpretation of the quantity $\frac{\nu_i}{1000}$ in this model?  What is the interpretation of $\lambda$ in this model? State the units for these quantities in both of your answers.

${\nu_i}$ means the total numbers of words in each chapter, $\frac{\nu_i}{1000}$ meansper 1000 words.

$\lambda$ is the average rate of the word _dark_ appears in a given chapter, and the unit for $\lambda$ is _dark_ per chapter

## 2b.

List the known and unknown variables and constants, as described in lecture 2.  Make sure your include $Y_1, ..., Y_n$, $y_1, ..., y_n$, $n$, $\lambda$, and $\nu_i$.


Known, Var $> 0$: $Y_1, ..., Y_n$

Known, Var $= 0$: $y_1, ..., y_n$, $n$, $\nu_i$

Unknown, Var $> 0$:

Unknown, Var $= 0$:, $\lambda$



## 2c.

Write down the likelihood in this new model.  Use this to calculate maximum likelihood estimator for $\lambda$.  Your answer should include the $\nu_i$'s.  

Suppose $\beta_i=\frac{\lambda v_i}{1000}$, then:

$$\mathcal{L} ( \lambda ) =\prod_{i=1}^{n} \frac{e^{-\beta_i} \beta_i  ^{y_i}}{y_i!}=  \frac{e^{-\sum\nolimits_{i=1}^{n} \beta_i} {\prod_{i=1}^{n} \beta_i^{ \sum\nolimits_{i=1}^{n} y_i}}}{ \prod_{i=1}^{n} y_i!}\propto e^{-\sum\nolimits_{i=1}^{n} \beta_i}{\prod_{i=1}^{n} \beta_i^{ \sum\nolimits_{i=1}^{n} y_i}}$$ 

Substitue back in $\frac{\lambda v_i}{1000}$ for $\beta_i$:

$$\mathcal{L} ( \lambda )\propto e^{-\sum\nolimits_{i=1}^{n} {\frac{\lambda v_i}{1000}}}{\prod_{i=1}^{n} {\frac{\lambda v_i}{1000}}^{ \sum\nolimits_{i=1}^{n} y_i}}$$
By taking the partial derivative in respect to $\lambda$ and equate it to 0:
$$ \frac{\mathrm{d} l({\lambda}) }{\mathrm{d} \lambda}=-\sum\nolimits_{i=1}^{n} \frac{v_i}{1000}+\frac{\sum\nolimits_{i=1}^{n}y_i}{\lambda} = 0$$

$$\hat \lambda= 1000\frac{\sum\nolimits_{i=1}^{n}y_i}{\sum\nolimits_{i=1}^{n}v_i}$$

## 2d.

Compute the maximum likelihood estimate and save it in the variable `lambda_mle`. In 1-2 sentences interpret its meaning (make sure you include units in your answers!).  

```{r}
# YOUR CODE HERE
lambda_mle = sum(dark_counts)/sum(chapter_lengths)*1000
lambda_mle
```

```{r}
. = ottr::check("tests/q2d.R")
```

The maximum likelihood estimate $\hat \lambda$ 0.9652801, in this scenario can be interpreted as the best estimate for the rate at which the word _dark_ occurs in each chapter every 1000 words is once. 

## 2e.

Plot the log-likelihood from the previous question in R using the data from on the frequency of _dark_ and the chapter lengths.  Add a vertical line at the value of `lambda_mle` to indicate the maximum likelihood.

```{r}
y=dark_counts
vi=chapter_lengths
L = function(lambda){#Log-Likelihood Function
  lambda * -sum((vi)/1000)+
  sum(y)*log(lambda*prod(vi/1000))
}


df_logLikelihood2 = data.frame(lambda = lambda_mle)
                              
ggplot(data = df_logLikelihood2, mapping = aes(x=lambda))+
  stat_function(fun = L)+
  xlim(0,5)+
  scale_y_continuous(name = "Likelihood")+
  geom_vline(xintercept = lambda_mle, color = 'red')
```


# Question 3
Correcting for chapter lengths is clearly an improvement, but we're still assuming that JKR uses the word _dark_ at the same rate in all chapters. In this problem we'll explore this assumption in more detail.

 
## 3a.
Why might it be unreasonable to assume that the rate of _dark_ usage is the same in all chapters? Comment in a few sentences.

This is probably not a good assumption since different chapters contain different elements that make up the overall plot, for example in some postive chapter may not use _dard_ at all, but some chapter scenes relavant with dark, then the rate of _dark_ will appear more often. 


## 3b.
We can use simulation to check our Poisson model, and in particular the assumption that the rate of _dark_ usage is the same in all chapters.  Generate simulated counts of the word _dark_ by sampling counts from a Poisson distribution with the rate $(\hat \lambda_{\text{MLE}} \nu_i) /1000$ for each chapter $i$. $\hat \lambda_{\text{MLE}}$ is the maximum likelihood estimate computing in 2d.  Store the vector of these values for each chapter in a variable of length 37 called `lambda_chapter`.  Make a side by side plot of the observed counts and simulated counts and note any similarities or differences (we've already created the observed histogram for you).  Are there any outliers in the observed data that don't seem to be reflected in the data simulated under our model?  

```{r}
observed_histogram <- ggplot(word_counts_mat) + geom_histogram(aes(x=dark)) +xlim(c(0, 25)) + ylim(c(0,10)) + ggtitle("Observed")


lambda_chapter <- rpois(37, (lambda_mle*chapter_lengths/1000)) 

simulated_counts <- tibble(dark = rpois(37, lambda_chapter))

simulated_histogram <- ggplot(simulated_counts) + geom_histogram(aes(x=dark)) +xlim(c(0, 25)) + ylim(c(0,10)) + ggtitle("Simulated")

## This uses the patchwork library to put the two plots side by side
observed_histogram + simulated_histogram
```

```{r}
. = ottr::check("tests/q3b.R")
```
In observed histogram, there are few outliers, while most of data are clustered in simulated data, and the overall range of observed data is larger than simulated data.

## 3c. Assume the word usage rate varies by chapter, that is, 
$$
p(Yi=y_i\mid \lambda, \nu_i, 1000) = \hbox{Poisson} (y_i \mid\lambda_i \cdot \frac{\nu_i}{1000}) \quad \hbox{ for } \quad i=1,...,n.
$$


Compute a separate maximum likelihood estimate of the rate of _dark_ usage (per 1000 words) in each chapter, $\hat \lambda_i$.  Make a bar plot of $\hat \lambda_i$ by chapter. Save the chapter-specific MLE in a vector of length 37 called `lambda_hats`.  Which chapter has the highest rate of usage of the word dark? Save the chapter number in a variable called `darkest_chapter`.


```{r, dependson="chapter_lengths"}
# Maximum likelihood estimate
lambda_hats <- (1000*dark_counts)/(chapter_lengths)

darkest_chapter <- 23 

# Make a bar plot of the MLEs, lambda_hats
barplot(lambda_hats, 
        main = "Rate of dark usage in each chapter",
        ylab = "Number of dark words",
        xlab = "Chapter")
```

```{r}
. = ottr::check("tests/q3c.R")
```


# Question 4

Let's go back to our original model for usage rates of the word _dark_.  You collect a random sample of book chapters penned by JKR and count how many times she uses the word _dark_ in each of the chapter in your sample, $(y_1,...,y_n)$.  In this set-up, $y_i$ is the  number of times the word _dark_ appeared in the $i$-th chapter, as before.  However, we will no longer assume that the rate of use of the word _dark_ is the same in every chapter.  Rather, we'll assume JKR uses the word _dark_ at different rates $\lambda_i$ in each chapter.   Naturally, this makes sense, since different chapters have different themes and tone.  To do this, we'll further assume that the rate of word usage $\lambda_i$ itself, is distributed according to a Gamma($\alpha$, $\beta$) with known parameters $\alpha$ and $\beta$,
  $$
   f(\Lambda=\lambda_i\mid \alpha,\beta) = \hbox{Gamma} (\lambda_i \mid\alpha,\beta).
  $$
and that $Y_i \sim \text{Pois}(\lambda_i)$ as in problem 1.  For now we will ignore any exposure parameters, $\nu_i$.  Note: this is a "warm up" to Bayesian inference, where it is standard to treat parameters as random variables and specify distributions for those parameters.  

## 4a.

Write out the the data generating process for the above model. 

The rate of word usage $\lambda_i$ is distributed according to $\hbox{Gamma} (\alpha,\beta)$, so based on the Gamma distrbution, we can generate $\lambda_i$.  $y_i$ is the number of times the word dark appeared in the $i$-th chapter, it is distributed according to $\hbox{Poisson} (\lambda_i)$. We can use the $\lambda_i$ from $\hbox{Gamma} (\alpha,\beta)$ to get $Y_i$.


## 4b. 

In R simulate 1000 values from the above data generating process, assume $\alpha=10$ (shape parameter of `rgamma`) and $\beta=1$ (rate parameter of `rgamma`).  Store the value in a vector of length 1000 called `counts`.  Compute the empirical mean and variance of values you generated.  For a Poisson distribution, the mean and the variance are the same.  In the following distribution is the variance greater than the mean (called ``overdispsersed'') or is the variance less than the mean (``underdispersed'')? Intuitively, why does this make sense?


```{r}
## Store simulated data in a vector of length 1000
lambda_dist <- rgamma(1000, shape=10,scale=1)
counts <- rpois(1000, lambda_dist)

print(mean(counts))
print(var(counts))
```

```{r}
. = ottr::check("tests/q4b.R")
```
The variance greater than the mean, so it is overdispsersed. Since $\lambda$ is a random variable in this case, so the variance will be greater. 


## 4c.

List the known and unknown variables and constants as described in lecture 2.  Make sure your table includes $Y_1, ..., Y_n$, $y_1, ..., y_n$, $n$, $\lambda$, $\alpha$, and $\beta$.  


Known, Var $> 0$: $Y_1, ..., Y_n$

Known, Var $= 0$: $y_1, ..., y_n$, $\alpha$, and $\beta$

Unknown, Var $> 0$: $\lambda$

Unknown, Var $= 0$:



# Extra Credit.
Compute $p(Y_i \mid \alpha, \beta) = \int p(Y_i, \lambda_i \mid \alpha, \beta) d\lambda_i$.  _Hint:_ The gamma function is defined as $\Gamma ( z ) = \int _ { 0 } ^ { \infty } x ^ { z - 1 } e ^ { - x } d x$. 

$$
\begin{aligned}
p(Y_i \mid \alpha, \beta) &= \int_{0}^{\infty} p(Y_i, \lambda_i \mid \alpha, \beta) d\lambda_i \\
 &= \int_{0}^{\infty} p(Y_{i} \mid \lambda_{i}) p(\lambda_{i} \mid \alpha,\beta) d\lambda_{i} \\
 &= \int_{0}^{\infty} \frac{-e^{-\lambda_{i}}\lambda_{i}^{y_{i}}}{y_{i}} 
 \frac{\lambda_{i}^{\alpha -1} \beta^{\alpha} e^{-\beta\lambda_{i}}}{\Gamma(\alpha)} d\lambda_{i} \\
 &= \int_{0}^{\infty} \frac{-\lambda_{i}^{y_{i}+\alpha-1}e^{-(1+\beta)\lambda_{i}}}{y_{i}!} 
 \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} d\lambda_{i} \\ 
 &= \int_{0}^{\infty} \frac{-\lambda_{i}^{y_{i}+\alpha-1}e^{-(1+\beta)\lambda_{i}}(\beta + 1)^{(a+y_{i})}}{\Gamma(y_{i}+\alpha)} d\lambda_{i} \frac{\Gamma(y_{i}+a)}{y_{i}!(\beta+1)^{(a+y_{i})}} \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} \\
 &= \frac{\Gamma(y_{i}+\alpha)}{y_{i}!(\beta+1)^{a+y_{i}}} \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} \\
 &= \frac{\Gamma(y_{i}+\alpha)}{\Gamma(y_{i}+1)\Gamma(\alpha)} \cdot \frac{\beta^{\alpha}}{(\beta+1)^(a+y_{i})} \\
 &= {\alpha + y_{i} - 1 \choose y_{i}} (\frac{\beta}{\beta+1})^{\alpha}(\frac{1}{\beta+1})^{y_{i}}, i = 1,2,3,...,n
\end{aligned}
$$

Hence is it is $$NB {(\alpha,\frac{\beta}{1+\beta})}$$


Fill in the blank.

You just showed that a Gamma mixture of Poisson distributions is a __ negative binomial distribution_${(\alpha,\frac{\beta}{1+\beta})}$____.  


